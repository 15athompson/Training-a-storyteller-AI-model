{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\python311\\lib\\site-packages (2.1.0+cu118)\n",
      "Requirement already satisfied: transformers in c:\\python311\\lib\\site-packages (4.34.1)\n",
      "Requirement already satisfied: pandas in c:\\python311\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: datasets in c:\\python311\\lib\\site-packages (2.14.6)\n",
      "Requirement already satisfied: filelock in c:\\python311\\lib\\site-packages (from torch) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in c:\\python311\\lib\\site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in c:\\python311\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\python311\\lib\\site-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\aidan_1k98io6\\appdata\\roaming\\python\\python311\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\python311\\lib\\site-packages (from torch) (2023.9.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\python311\\lib\\site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\python311\\lib\\site-packages (from transformers) (1.24.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\python311\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\aidan_1k98io6\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\python311\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\aidan_1k98io6\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in c:\\python311\\lib\\site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\python311\\lib\\site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\python311\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aidan_1k98io6\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python311\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\python311\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\python311\\lib\\site-packages (from datasets) (13.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\python311\\lib\\site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: xxhash in c:\\python311\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\python311\\lib\\site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in c:\\python311\\lib\\site-packages (from datasets) (3.8.6)\n",
      "Requirement already satisfied: accelerate>=0.20.3 in c:\\python311\\lib\\site-packages (from transformers) (0.23.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\aidan_1k98io6\\appdata\\roaming\\python\\python311\\site-packages (from accelerate>=0.20.3->transformers) (5.9.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\python311\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\aidan_1k98io6\\appdata\\roaming\\python\\python311\\site-packages (from aiohttp->datasets) (3.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\python311\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\python311\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\python311\\lib\\site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\python311\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\python311\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aidan_1k98io6\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aidan_1k98io6\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python311\\lib\\site-packages (from requests->transformers) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aidan_1k98io6\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\aidan_1k98io6\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\aidan_1k98io6\\appdata\\roaming\\python\\python311\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\python311\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\python311\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# install all requirements via pip\n",
    "!pip install torch transformers pandas datasets transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialise GPT2 model (importing Hugging Face)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"distilgpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Once upon a time of opportunity, I found myself thinking, \"Well, I’ve always done this with, and I thought, what about it? If I was a musician who had always been a musician, it might sound interesting to me but… I’ve never been quite so good at it.” I am so honored to say I feel like a musician at all. It’t just me and my community as a whole, but this particular part of me is not just me,']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test out the base GPT2 model\n",
    "\n",
    "prompt = \"Once upon a time\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(inputs.input_ids, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)\n",
    "\n",
    "output_string = tokenizer.batch_decode(outputs)\n",
    "\n",
    "output_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5d4024783064c1b94311eb75ba54e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91af3e7903704ff0b2f42e9843dac86b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d89b628153d4482bf62dad6f1b7463f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/249M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "055c249691d44a7e829ece14646fe275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/248M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cb2a373e32c4804b7ddc784fa7e8c09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/246M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68690963f9e441dfbe5fc31a8ce7cb3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/248M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7378967cea1044e6956b1f38f1785283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/9.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c302e24160d4a7f9e410fa5198dcda1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb1c415e91c406aa8f16c58522f19de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc48e0d87e704fbe8e8d4fb1346b3a57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 2119719\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 21990\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import our dataset from hugging face\n",
    "from datasets import load_dataset\n",
    "\n",
    "short_stories_dataset = load_dataset(\"roneneldan/TinyStories\")\n",
    "short_stories_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'One day, a little fish named Fin was swimming near the shore. He saw a big crab and wanted to be friends. \"Hi, I am Fin. Do you want to play?\" asked the little fish. The crab looked at Fin and said, \"No, I don\\'t want to play. I am cold and I don\\'t feel fine.\"\\n\\nFin felt sad but wanted to help the crab feel better. He swam away and thought of a plan. He remembered that the sun could make things warm. So, Fin swam to the top of the water and called to the sun, \"Please, sun, help my new friend feel fine and not freeze!\"\\n\\nThe sun heard Fin\\'s call and shone its warm light on the shore. The crab started to feel better and not so cold. He saw Fin and said, \"Thank you, little fish, for making me feel fine. I don\\'t feel like I will freeze now. Let\\'s play together!\" And so, Fin and the crab played and became good friends.'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_stories_dataset[\"train\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 800\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a smaller subset of data from the dataset\n",
    "\n",
    "small_story_dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train[:1000]\")\n",
    "small_story_dataset = small_story_dataset.train_test_split(train_size=0.8)\n",
    "\n",
    "small_story_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[220,\n",
       " 521,\n",
       " 360,\n",
       " 150,\n",
       " 103,\n",
       " 167,\n",
       " 205,\n",
       " 354,\n",
       " 470,\n",
       " 148,\n",
       " 135,\n",
       " 194,\n",
       " 186,\n",
       " 387,\n",
       " 142,\n",
       " 120,\n",
       " 278,\n",
       " 138,\n",
       " 99,\n",
       " 179,\n",
       " 522,\n",
       " 165,\n",
       " 129,\n",
       " 149,\n",
       " 144,\n",
       " 299,\n",
       " 116,\n",
       " 229,\n",
       " 280,\n",
       " 161,\n",
       " 189,\n",
       " 157,\n",
       " 265,\n",
       " 151,\n",
       " 157,\n",
       " 183,\n",
       " 241,\n",
       " 162,\n",
       " 174,\n",
       " 138,\n",
       " 135,\n",
       " 112,\n",
       " 195,\n",
       " 150,\n",
       " 188,\n",
       " 155,\n",
       " 145,\n",
       " 153,\n",
       " 166,\n",
       " 136,\n",
       " 215,\n",
       " 168,\n",
       " 161,\n",
       " 157,\n",
       " 119,\n",
       " 97,\n",
       " 174,\n",
       " 216,\n",
       " 143,\n",
       " 107,\n",
       " 265,\n",
       " 162,\n",
       " 171,\n",
       " 129,\n",
       " 120,\n",
       " 127,\n",
       " 396,\n",
       " 167,\n",
       " 140,\n",
       " 112,\n",
       " 154,\n",
       " 111,\n",
       " 125,\n",
       " 174,\n",
       " 106,\n",
       " 194,\n",
       " 104,\n",
       " 255,\n",
       " 117,\n",
       " 411,\n",
       " 177,\n",
       " 142,\n",
       " 142,\n",
       " 305,\n",
       " 181,\n",
       " 328,\n",
       " 278,\n",
       " 171,\n",
       " 281,\n",
       " 167,\n",
       " 173,\n",
       " 108,\n",
       " 165,\n",
       " 149,\n",
       " 113,\n",
       " 167,\n",
       " 151,\n",
       " 117,\n",
       " 139,\n",
       " 140,\n",
       " 215,\n",
       " 179,\n",
       " 137,\n",
       " 140,\n",
       " 155,\n",
       " 187,\n",
       " 188,\n",
       " 324,\n",
       " 115,\n",
       " 116,\n",
       " 150,\n",
       " 104,\n",
       " 127,\n",
       " 171,\n",
       " 197,\n",
       " 170,\n",
       " 123,\n",
       " 215,\n",
       " 152,\n",
       " 215,\n",
       " 185,\n",
       " 309,\n",
       " 136,\n",
       " 159,\n",
       " 147,\n",
       " 138,\n",
       " 102,\n",
       " 157,\n",
       " 179,\n",
       " 179,\n",
       " 161,\n",
       " 127,\n",
       " 154,\n",
       " 152,\n",
       " 158,\n",
       " 94,\n",
       " 110,\n",
       " 117,\n",
       " 133,\n",
       " 149,\n",
       " 79,\n",
       " 276,\n",
       " 188,\n",
       " 196,\n",
       " 110,\n",
       " 138,\n",
       " 161,\n",
       " 176,\n",
       " 176,\n",
       " 150,\n",
       " 146,\n",
       " 142,\n",
       " 220,\n",
       " 318,\n",
       " 181,\n",
       " 185,\n",
       " 141,\n",
       " 226,\n",
       " 158,\n",
       " 328,\n",
       " 143,\n",
       " 151,\n",
       " 150,\n",
       " 342,\n",
       " 132,\n",
       " 176,\n",
       " 179,\n",
       " 285,\n",
       " 260,\n",
       " 156,\n",
       " 110,\n",
       " 156,\n",
       " 145,\n",
       " 211,\n",
       " 142,\n",
       " 587,\n",
       " 133,\n",
       " 115,\n",
       " 112,\n",
       " 134,\n",
       " 140,\n",
       " 141,\n",
       " 117,\n",
       " 179,\n",
       " 165,\n",
       " 109,\n",
       " 166,\n",
       " 157,\n",
       " 319,\n",
       " 182,\n",
       " 290,\n",
       " 127,\n",
       " 129,\n",
       " 152,\n",
       " 160,\n",
       " 118,\n",
       " 86,\n",
       " 128,\n",
       " 154,\n",
       " 141,\n",
       " 132,\n",
       " 147,\n",
       " 131,\n",
       " 166,\n",
       " 140,\n",
       " 124,\n",
       " 82,\n",
       " 134,\n",
       " 184,\n",
       " 163,\n",
       " 583,\n",
       " 158,\n",
       " 258,\n",
       " 122,\n",
       " 462,\n",
       " 176,\n",
       " 80,\n",
       " 206,\n",
       " 329,\n",
       " 183,\n",
       " 120,\n",
       " 144,\n",
       " 147,\n",
       " 307,\n",
       " 188,\n",
       " 141,\n",
       " 149,\n",
       " 137,\n",
       " 105,\n",
       " 136,\n",
       " 156,\n",
       " 433,\n",
       " 157,\n",
       " 131,\n",
       " 149,\n",
       " 203,\n",
       " 171,\n",
       " 194,\n",
       " 158,\n",
       " 131,\n",
       " 365,\n",
       " 116,\n",
       " 194,\n",
       " 190,\n",
       " 214,\n",
       " 138,\n",
       " 155,\n",
       " 101,\n",
       " 197,\n",
       " 378,\n",
       " 127,\n",
       " 197,\n",
       " 150,\n",
       " 140,\n",
       " 142,\n",
       " 116,\n",
       " 125,\n",
       " 148,\n",
       " 218,\n",
       " 134,\n",
       " 196,\n",
       " 141,\n",
       " 150,\n",
       " 137,\n",
       " 164,\n",
       " 218,\n",
       " 821,\n",
       " 172,\n",
       " 154,\n",
       " 127,\n",
       " 263,\n",
       " 150,\n",
       " 135,\n",
       " 189,\n",
       " 141,\n",
       " 201,\n",
       " 141,\n",
       " 158,\n",
       " 441,\n",
       " 171,\n",
       " 123,\n",
       " 125,\n",
       " 104,\n",
       " 144,\n",
       " 169,\n",
       " 111,\n",
       " 110,\n",
       " 94,\n",
       " 296,\n",
       " 145,\n",
       " 167,\n",
       " 131,\n",
       " 318,\n",
       " 97,\n",
       " 177,\n",
       " 144,\n",
       " 196,\n",
       " 167,\n",
       " 147,\n",
       " 226,\n",
       " 208,\n",
       " 138,\n",
       " 188,\n",
       " 208,\n",
       " 110,\n",
       " 201,\n",
       " 116,\n",
       " 152,\n",
       " 132,\n",
       " 236,\n",
       " 186,\n",
       " 132,\n",
       " 365,\n",
       " 143,\n",
       " 145,\n",
       " 119,\n",
       " 133,\n",
       " 298,\n",
       " 136,\n",
       " 397,\n",
       " 195,\n",
       " 110,\n",
       " 158,\n",
       " 447,\n",
       " 116,\n",
       " 189,\n",
       " 128,\n",
       " 198,\n",
       " 160,\n",
       " 175,\n",
       " 106,\n",
       " 206,\n",
       " 144,\n",
       " 276,\n",
       " 204,\n",
       " 139,\n",
       " 176,\n",
       " 168,\n",
       " 162,\n",
       " 154,\n",
       " 104,\n",
       " 158,\n",
       " 119,\n",
       " 210,\n",
       " 118,\n",
       " 240,\n",
       " 262,\n",
       " 94,\n",
       " 116,\n",
       " 151,\n",
       " 190,\n",
       " 178,\n",
       " 119,\n",
       " 130,\n",
       " 125,\n",
       " 300,\n",
       " 315,\n",
       " 220,\n",
       " 163,\n",
       " 123,\n",
       " 147,\n",
       " 147,\n",
       " 268,\n",
       " 462,\n",
       " 435,\n",
       " 522,\n",
       " 191,\n",
       " 151,\n",
       " 132,\n",
       " 144,\n",
       " 374,\n",
       " 323,\n",
       " 84,\n",
       " 132,\n",
       " 107,\n",
       " 323,\n",
       " 155,\n",
       " 267,\n",
       " 233,\n",
       " 192,\n",
       " 120,\n",
       " 126,\n",
       " 172,\n",
       " 296,\n",
       " 129,\n",
       " 160,\n",
       " 176,\n",
       " 141,\n",
       " 141,\n",
       " 164,\n",
       " 114,\n",
       " 192,\n",
       " 177,\n",
       " 141,\n",
       " 232,\n",
       " 203,\n",
       " 177,\n",
       " 344,\n",
       " 170,\n",
       " 151,\n",
       " 184,\n",
       " 134,\n",
       " 272,\n",
       " 177,\n",
       " 164,\n",
       " 188,\n",
       " 320,\n",
       " 99,\n",
       " 164,\n",
       " 162,\n",
       " 137,\n",
       " 117,\n",
       " 221,\n",
       " 262,\n",
       " 170,\n",
       " 135,\n",
       " 137,\n",
       " 105,\n",
       " 193,\n",
       " 145,\n",
       " 166,\n",
       " 148,\n",
       " 333,\n",
       " 99,\n",
       " 139,\n",
       " 102,\n",
       " 142,\n",
       " 120,\n",
       " 162,\n",
       " 143,\n",
       " 124,\n",
       " 202,\n",
       " 160,\n",
       " 130,\n",
       " 203,\n",
       " 128,\n",
       " 171,\n",
       " 146,\n",
       " 212,\n",
       " 202,\n",
       " 148,\n",
       " 119,\n",
       " 184,\n",
       " 149,\n",
       " 167,\n",
       " 366,\n",
       " 194,\n",
       " 184,\n",
       " 128,\n",
       " 149,\n",
       " 148,\n",
       " 137,\n",
       " 137,\n",
       " 146,\n",
       " 160,\n",
       " 136,\n",
       " 148,\n",
       " 112,\n",
       " 247,\n",
       " 126,\n",
       " 157,\n",
       " 106,\n",
       " 141,\n",
       " 155,\n",
       " 144,\n",
       " 338,\n",
       " 90,\n",
       " 318,\n",
       " 179,\n",
       " 153,\n",
       " 137,\n",
       " 133,\n",
       " 155,\n",
       " 198,\n",
       " 276,\n",
       " 146,\n",
       " 138,\n",
       " 125,\n",
       " 120,\n",
       " 150,\n",
       " 219,\n",
       " 164,\n",
       " 140,\n",
       " 138,\n",
       " 116,\n",
       " 176,\n",
       " 139,\n",
       " 187,\n",
       " 114,\n",
       " 122,\n",
       " 224,\n",
       " 140,\n",
       " 272,\n",
       " 151,\n",
       " 170,\n",
       " 123,\n",
       " 368,\n",
       " 129,\n",
       " 107,\n",
       " 192,\n",
       " 200,\n",
       " 154,\n",
       " 140,\n",
       " 360,\n",
       " 144,\n",
       " 233,\n",
       " 187,\n",
       " 134,\n",
       " 385,\n",
       " 202,\n",
       " 189,\n",
       " 137,\n",
       " 144,\n",
       " 162,\n",
       " 160,\n",
       " 150,\n",
       " 137,\n",
       " 170,\n",
       " 152,\n",
       " 118,\n",
       " 414,\n",
       " 216,\n",
       " 289,\n",
       " 138,\n",
       " 104,\n",
       " 126,\n",
       " 155,\n",
       " 122,\n",
       " 395,\n",
       " 224,\n",
       " 144,\n",
       " 180,\n",
       " 244,\n",
       " 202,\n",
       " 184,\n",
       " 150,\n",
       " 298,\n",
       " 160,\n",
       " 349,\n",
       " 142,\n",
       " 198,\n",
       " 124,\n",
       " 184,\n",
       " 177,\n",
       " 151,\n",
       " 203,\n",
       " 332,\n",
       " 451,\n",
       " 110,\n",
       " 277,\n",
       " 205,\n",
       " 155,\n",
       " 148,\n",
       " 227,\n",
       " 135,\n",
       " 263,\n",
       " 168,\n",
       " 175,\n",
       " 123,\n",
       " 296,\n",
       " 215,\n",
       " 107,\n",
       " 209,\n",
       " 127,\n",
       " 170,\n",
       " 305,\n",
       " 160,\n",
       " 132,\n",
       " 156,\n",
       " 148,\n",
       " 104,\n",
       " 212,\n",
       " 137,\n",
       " 101,\n",
       " 144,\n",
       " 101,\n",
       " 116,\n",
       " 140,\n",
       " 298,\n",
       " 141,\n",
       " 112,\n",
       " 141,\n",
       " 129,\n",
       " 253,\n",
       " 193,\n",
       " 477,\n",
       " 140,\n",
       " 165,\n",
       " 165,\n",
       " 208,\n",
       " 114,\n",
       " 136,\n",
       " 146,\n",
       " 151,\n",
       " 134,\n",
       " 271,\n",
       " 130,\n",
       " 243,\n",
       " 285,\n",
       " 196,\n",
       " 164,\n",
       " 139,\n",
       " 310,\n",
       " 109,\n",
       " 165,\n",
       " 413,\n",
       " 140,\n",
       " 130,\n",
       " 140,\n",
       " 157,\n",
       " 157,\n",
       " 155,\n",
       " 104,\n",
       " 190,\n",
       " 175,\n",
       " 125,\n",
       " 341,\n",
       " 311,\n",
       " 147,\n",
       " 124,\n",
       " 174,\n",
       " 107,\n",
       " 330,\n",
       " 478,\n",
       " 166,\n",
       " 134,\n",
       " 158,\n",
       " 147,\n",
       " 107,\n",
       " 125,\n",
       " 157,\n",
       " 103,\n",
       " 138,\n",
       " 217,\n",
       " 115,\n",
       " 163,\n",
       " 286,\n",
       " 198,\n",
       " 130,\n",
       " 192,\n",
       " 116,\n",
       " 136,\n",
       " 113,\n",
       " 170,\n",
       " 110,\n",
       " 101,\n",
       " 138,\n",
       " 199,\n",
       " 173,\n",
       " 257,\n",
       " 139,\n",
       " 115,\n",
       " 145,\n",
       " 156,\n",
       " 493,\n",
       " 97,\n",
       " 173,\n",
       " 163,\n",
       " 143,\n",
       " 155,\n",
       " 139,\n",
       " 141,\n",
       " 136,\n",
       " 145,\n",
       " 138,\n",
       " 150,\n",
       " 172,\n",
       " 183,\n",
       " 109,\n",
       " 172,\n",
       " 136,\n",
       " 596,\n",
       " 159,\n",
       " 217,\n",
       " 114,\n",
       " 159,\n",
       " 193,\n",
       " 117,\n",
       " 418,\n",
       " 123,\n",
       " 156,\n",
       " 207,\n",
       " 309,\n",
       " 293,\n",
       " 145,\n",
       " 150,\n",
       " 132,\n",
       " 166,\n",
       " 151,\n",
       " 183,\n",
       " 191,\n",
       " 154,\n",
       " 151,\n",
       " 132,\n",
       " 145,\n",
       " 333,\n",
       " 150,\n",
       " 149,\n",
       " 118,\n",
       " 325,\n",
       " 137,\n",
       " 176,\n",
       " 119,\n",
       " 119,\n",
       " 94,\n",
       " 272,\n",
       " 232,\n",
       " 61,\n",
       " 203,\n",
       " 124,\n",
       " 232,\n",
       " 120,\n",
       " 218,\n",
       " 150,\n",
       " 177,\n",
       " 163,\n",
       " 453,\n",
       " 276,\n",
       " 171,\n",
       " 159,\n",
       " 349,\n",
       " 105,\n",
       " 411,\n",
       " 72,\n",
       " 134,\n",
       " 277,\n",
       " 142,\n",
       " 125,\n",
       " 160,\n",
       " 322,\n",
       " 183,\n",
       " 189,\n",
       " 120,\n",
       " 152,\n",
       " 128,\n",
       " 105,\n",
       " 128,\n",
       " 152,\n",
       " 153,\n",
       " 164,\n",
       " 168,\n",
       " 159,\n",
       " 102,\n",
       " 180,\n",
       " 143,\n",
       " 181,\n",
       " 106,\n",
       " 118,\n",
       " 159,\n",
       " 158,\n",
       " 121,\n",
       " 165,\n",
       " 140,\n",
       " 181,\n",
       " 122,\n",
       " 84,\n",
       " 182,\n",
       " 123,\n",
       " 118,\n",
       " 140,\n",
       " 236,\n",
       " 181,\n",
       " 155,\n",
       " 170,\n",
       " 159,\n",
       " 391,\n",
       " 145,\n",
       " 188,\n",
       " 161,\n",
       " 118,\n",
       " 442,\n",
       " 142,\n",
       " 161,\n",
       " 312,\n",
       " 166,\n",
       " 221,\n",
       " 147,\n",
       " 161,\n",
       " 157,\n",
       " 136,\n",
       " 190,\n",
       " 125,\n",
       " 137,\n",
       " 137,\n",
       " 246,\n",
       " 288,\n",
       " 198,\n",
       " 89,\n",
       " 146,\n",
       " 130,\n",
       " 118,\n",
       " 129,\n",
       " 367,\n",
       " 200,\n",
       " 121,\n",
       " 151,\n",
       " 118,\n",
       " 181,\n",
       " 178,\n",
       " 145]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(x[\"text\"].split(\" \")) for x in small_story_dataset[\"train\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bee79cc8d70644a0b0df43f8e7de7fbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c864a5f8033d42b6ab6709a80d641fd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 800\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize our dataset\n",
    "def preprocess_batch(batch):\n",
    "    all_text_items = batch[\"text\"]\n",
    "    trimmed_text_items = [x[:500] for x in all_text_items]\n",
    "    return tokenizer(trimmed_text_items)\n",
    "\n",
    "tokenized_dataset = small_story_dataset.map(\n",
    "    preprocess_batch, \n",
    "    batched=True, \n",
    "    batch_size=10, \n",
    "    remove_columns=small_story_dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "tokenized_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [7454,\n",
       "  2402,\n",
       "  257,\n",
       "  640,\n",
       "  11,\n",
       "  612,\n",
       "  373,\n",
       "  257,\n",
       "  1310,\n",
       "  2576,\n",
       "  3706,\n",
       "  3619,\n",
       "  88,\n",
       "  13,\n",
       "  3619,\n",
       "  88,\n",
       "  6151,\n",
       "  2712,\n",
       "  351,\n",
       "  607,\n",
       "  15392,\n",
       "  13,\n",
       "  1375,\n",
       "  8288,\n",
       "  284,\n",
       "  6129,\n",
       "  340,\n",
       "  1029,\n",
       "  287,\n",
       "  262,\n",
       "  6766,\n",
       "  290,\n",
       "  7906,\n",
       "  1088,\n",
       "  290,\n",
       "  1088,\n",
       "  13,\n",
       "  220,\n",
       "  198,\n",
       "  198,\n",
       "  3198,\n",
       "  1110,\n",
       "  11,\n",
       "  3619,\n",
       "  88,\n",
       "  2227,\n",
       "  284,\n",
       "  1011,\n",
       "  607,\n",
       "  15392,\n",
       "  329,\n",
       "  257,\n",
       "  6594,\n",
       "  1088,\n",
       "  262,\n",
       "  3952,\n",
       "  13,\n",
       "  1375,\n",
       "  1234,\n",
       "  319,\n",
       "  607,\n",
       "  10012,\n",
       "  290,\n",
       "  572,\n",
       "  673,\n",
       "  1816,\n",
       "  13,\n",
       "  198,\n",
       "  198,\n",
       "  2953,\n",
       "  262,\n",
       "  3952,\n",
       "  11,\n",
       "  3619,\n",
       "  88,\n",
       "  373,\n",
       "  13226,\n",
       "  7348,\n",
       "  287,\n",
       "  262,\n",
       "  6766,\n",
       "  618,\n",
       "  477,\n",
       "  286,\n",
       "  257,\n",
       "  4802,\n",
       "  11,\n",
       "  257,\n",
       "  1263,\n",
       "  35253,\n",
       "  286,\n",
       "  2344,\n",
       "  1625,\n",
       "  290,\n",
       "  607,\n",
       "  15392,\n",
       "  11406,\n",
       "  287,\n",
       "  257,\n",
       "  16723,\n",
       "  13,\n",
       "  198,\n",
       "  198,\n",
       "  14295,\n",
       "  88,\n",
       "  373,\n",
       "  845,\n",
       "  9247,\n",
       "  290,\n",
       "  2067,\n",
       "  284,\n",
       "  3960,\n",
       "  13,\n",
       "  24975,\n",
       "  11,\n",
       "  281,\n",
       "  10966,\n",
       "  8223,\n",
       "  1625,\n",
       "  7976,\n",
       "  15816,\n",
       "  625,\n",
       "  220],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForLanguageModeling(tokenizer=GPT2TokenizerFast(name_or_path='distilgpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}, mlm=False, mlm_probability=0.15, pad_to_multiple_of=None, tf_experimental_compile=False, return_tensors='pt')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Data Collator\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our Trainer and TrainingArguments\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=10\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    args=training_args,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
